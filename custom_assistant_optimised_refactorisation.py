# TODO (in order of priority):
# BTW, some message generated by the llm, sent on discord, aren't keept in context window- fix that
# loop function (decided from model) to fix code (web is done)
# Let the bot add reaction and remove them to a message

import tool
import memory_tool

import os
import re
import socket
import subprocess
import sys
from datetime import datetime, timedelta
import asyncio

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ["OLLAMA_MAX_LOADED_MODELS"] = "2"

import discord
from discord import Activity, ActivityType, Status, app_commands

import torch
from ollama import Client
from torch import nn
from transformers import DistilBertTokenizer, DistilBertModel

# Quick prerequisites check
def check_port(port): 
    try: socket.create_connection(('localhost', port), 1); return True
    except: return False

if not check_port(9872): 
    if input("TTS WebUI not running. Start it? (y/N): ").lower() == 'y': subprocess.Popen([sys.executable, 'GPT/webui.py'])

if not any('ollama' in line for line in os.popen('ps aux' if os.name != 'nt' else 'tasklist').read().split('\n')):
    if input("Ollama not running. Start it? (y/n): ").lower() == 'y': subprocess.Popen(['ollama', 'serve'])
    else: print("Please run 'ollama serve'"); sys.exit(1)

print(f"OLLAMA_FLASH_ATTENTION: {os.environ.get('OLLAMA_FLASH_ATTENTION', 'false')}")
if os.environ.get('OLLAMA_FLASH_ATTENTION', 'false') == False:
    print("(set to 'true' for faster inference)")

# Define the ImportanceRegressor class
class ImportanceRegressor(nn.Module):
    def __init__(self):
        super(ImportanceRegressor, self).__init__()
        self.transformer = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.regression_head = nn.Linear(self.transformer.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]
        importance_score = self.regression_head(cls_output)
        return importance_score

# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
importance_model = ImportanceRegressor()

# Fix state_dict keys mismatch
state_dict = torch.load("importance_model.pth", map_location=torch.device('cuda'))
new_state_dict = {}
for key in state_dict:
    new_key = key.replace("regressor.", "regression_head.")
    new_state_dict[new_key] = state_dict[key]
importance_model.load_state_dict(new_state_dict)

importance_model.eval()

IMPORTANCE_THRESHOLD = 0.47
THINK_TRIGGER = 60 # number of letter
OLLAMA_MODEL = "" #Recommanded model: qwen3:8b or qwen3:14b-q4_K_M
LINK = "https://localhost:11434" # Your link

# Initialize Ollama client
ollama_client = Client(
    host=LINK,
)

MODEL_INFO = ollama_client._request_raw("POST", "/api/show",json={"name":OLLAMA_MODEL}).json()
MODEL_FAMILY = MODEL_INFO["details"]["family"]
MODEL_CAPABILITIES = MODEL_INFO["capabilities"]
print(f"Model Family: '{MODEL_FAMILY}', Model Capabilities: '{MODEL_CAPABILITIES}'")

# Discord bot token // Could be securised, but its not as its still a prototype
TOKEN = ""

# Get current script directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
IMG_DIR = os.path.join(BASE_DIR, "img")
DOC_DIR = os.path.join(BASE_DIR, "doc")

Analyse = False
Voice_Message = False

conversation_history = tool.load_conversation_history()
touched_channels = set()

ATTACHMENT_FOLDER = "attachments"
if not os.path.exists(ATTACHMENT_FOLDER):
    os.makedirs(ATTACHMENT_FOLDER)

# Create a Discord client
intents = discord.Intents.default()
intents.messages = True
intents.reactions = True
intents.members = True
intents.message_content = True
client = discord.Client(intents=intents)
tree = app_commands.CommandTree(client)

# List to hold concatenated prompts for low-importance messages
low_importance_messages = []

# Slash command to clear conversation history
@tree.command(name="clear", description="Clear the conversation history for the current channel")
async def clear(interaction: discord.Interaction):
    channel_id = interaction.channel.id
    conversation_history[channel_id] = []
    await interaction.response.send_message("Conversation history cleared!", ephemeral=True)

@client.event
async def on_ready():
    await tree.sync()  # Sync the slash commands
    print(f"Logged in as {client.user}")

@client.event
async def on_message(message):
    channel_id = str(message.channel.id)

    # Initialize channel if needed
    if channel_id not in conversation_history:
        conversation_history[channel_id] = []

    # This ensures the logic runs only once per reboot per channel
    if channel_id not in touched_channels and conversation_history[channel_id]:
        touched_channels.add(channel_id)  # mark as accessed

        # Look for last user message to calculate offline duration
        for entry in reversed(conversation_history[channel_id]):
            if entry['role'] == 'user':
                match = re.match(r"(\d{2}:\d{2})", entry['content'])
                if match:
                    last_time_str = match.group(1)
                    now_time = datetime.now().strftime("%H:%M")

                    last_time = datetime.strptime(last_time_str, "%H:%M")
                    current_time = datetime.strptime(now_time, "%H:%M")
                    if current_time < last_time:
                        current_time += timedelta(days=1)

                    time_spent = str(current_time - last_time)
                    conversation_history[channel_id].append({
                        'role': 'system',
                        'content': f"The bot was offline for {time_spent}."
                    })
                break

    date_time = datetime.now().strftime("%H:%M") # time format exemple: 09:42

    if message.author == client.user:
        return
    
    os.system('cls' if os.name == 'nt' else 'clear')

    content = message.content

    # Replace @<user> from message.content with the actual nickname, usefull for llm interaction
    if message.mentions:
        for user in message.mentions:
            content = content.replace(f"<@{user.id}>", f"@{user.name}")

    prompt = f"{date_time} - {message.author}: {content}"

    file_attachments = []

    Voice_Message = None
    if message.attachments:
        for attachment in message.attachments:
            # Download each attachment
            file_path = os.path.join(ATTACHMENT_FOLDER, attachment.filename)
            
            # Save file locally
            with open(file_path, "wb") as file:
                await attachment.save(file)

            # Add absolute path to list
            if "vision" in MODEL_CAPABILITIES and ".png" or ".jpg" in file_path:
                conversation_history[channel_id].append({
                    'role': 'user',
                    'content': attachment.filename,
                    'images': file_path,
                })
            else:
                file_attachments.append(os.path.abspath(file_path))
        print(file_attachments, LINK, message.content)

        analyse = tool.analyse(file_attachments, LINK)
        print(analyse)
        if 'audio_transcriptions' in analyse:
            Voice_Message = True
    else:
        analyse = False
        Voice_Message = False

    # Tokenize and process the message
    inputs = tokenizer(message.content, return_tensors="pt", truncation=True, padding=True, max_length=512)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    
    # Predict importance score
    with torch.no_grad():
        score = importance_model(input_ids=input_ids, attention_mask=attention_mask).item()
    
    if "/silent" in message.content:
        return
    
    if analyse:
        score = 1
        conversation_history[channel_id].append({
            'role': 'system',
            'content': f"{message.author} sent an attachment, here's the information about it: {analyse}"
        })

        if Voice_Message:
            conversation_history[channel_id].append({
                'role': 'user',
                'content': f"{message.author}'s voice message transcription: {analyse}"
            })
    
    if message.reference:
        score = 1
        replied_message = await message.channel.fetch_message(message.reference.message_id)
        replied_content = replied_message.content
        replied_author = replied_message.author
        replied_attachments = replied_message.attachments  # Keep original attachment objects

        file_attachments = []

        if replied_attachments:
            for attachment in replied_attachments:  # Iterate over Attachment objects
                file_path = os.path.join(ATTACHMENT_FOLDER, attachment.filename)

                # Save file locally
                with open(file_path, "wb") as file:
                    await attachment.save(file)

                # Add absolute path to list
                file_attachments.append(os.path.abspath(file_path))

            analyse = tool.analyse(file_attachments, LINK)
            print(analyse)

            if 'audio_transcriptions' in analyse:
                Voice_Message = True

            replied_content = f"{analyse}. {replied_content}"
        else:
            analyse = False
            Voice_Message = False

        conversation_history[channel_id].append({
            'role': 'system',
            'content': f"{message.author} replied to a message sent by {replied_author}: {replied_content}"
        })
    
    if MODEL_FAMILY == 'qwen3':
        # Allow model CoT for longer message, keeping reply speed faster for short chat.
        if len(message.content) < THINK_TRIGGER:
            prompt += "\n/no_think"
        else:
            prompt += "\n/think"

    # Append the current user message to the channel's conversation history
    conversation_history[channel_id].append({
        'role': 'user',
        'content': prompt
    })
    # If importance score exceeds threshold, query Ollama
    if score > IMPORTANCE_THRESHOLD:
        # Combine any previous low-importance messages
        if low_importance_messages:
            prompt = ". ".join(low_importance_messages) + ". " + prompt
            low_importance_messages.clear()  # Clear after combining
    
        # Add previous conversation history from the channel to Ollama request
        ollama_messages = conversation_history[channel_id].copy()
        
        try:
            # Send a typing indicator
            async with message.channel.typing():
                # Configure temperature and top-p
                response = await asyncio.to_thread(ollama_client.chat, 
                    model=OLLAMA_MODEL, 
                    messages=ollama_messages,
                    tools=[
                        {
                            'type': 'function',
                            'function': {
                                'name': 'browse',
                                'description': 'Get online information',
                                'parameters': {
                                    'type': 'object',
                                    'properties': {
                                        'reply': {
                                            'type': 'string',
                                            'description': 'Respond to the user about what you will browse',
                                        },
                                        'type': {
                                            'type': 'string',
                                            "enum": ["web", "gif", "youtube"],
                                            'description': 'Specify where to search information, either web, gif or youtube',
                                        },
                                        'query': {
                                            'type': 'string',
                                            'description': 'The browse query. LINKs works too',
                                        },
                                    },
                                    'required': ['reply', 'type', 'query'],
                                },
                            },
                        },
                        {
                            'type': 'function',
                            'function': {
                                'name': 'python',
                                'description': 'run short python script',
                                'parameters': {
                                    'type': 'object',
                                    'properties': {
                                        'script': {
                                            'type': 'string',
                                            'description': 'the python script. print the result',
                                        },
                                        'reply': {
                                            'type': 'string',
                                            'description': 'Respond to the user about what you are trying to write.',
                                        },
                                    },
                                    'required': ['reply', 'script'],
                                },
                            },
                        },
                        {
                          "type": "function",
                          "function": {
                            "name": "memory",
                            "description": "Handles user memory by either remembering past information or memorizing new data.",
                            "parameters": {
                              "type": "object",
                              "properties": {
                                "type": {
                                  "type": "string",
                                  "enum": ["memorize", "remember"],
                                  "description": "Specify whether to 'memorize' new data or 'remember' past data.",
                                },
                                "username": {
                                  "type": "string",
                                  "description": "The user's name or identifier.",
                                },
                                "query": {
                                  "type": "string",
                                  "description": "For 'memorize', this is the data to save. For 'remember', this is the keyword to search for precise memory data.",
                                },
                                "reply": {
                                  "type": "string",
                                  "description": "respond to the user message.",
                                }
                              },
                              "required": ["type", "username", "query", 'reply']
                            }
                          }
                        },
                    ],
                    options={
                        'temperature': 0.5,  # Controls randomness; 0.0 = deterministic, higher = more random
                        'top_p': 0.9,        # Nucleus sampling; considers tokens with cumulative probability <= top_p
                        'top_k': 25          # IDK but chatGPT told me to add this line too
                        #'keep_alive': -1    # Need to be fixed
                    }
                )

            tool_calls = response.get('message', {}).get('tool_calls', [])
            if tool_calls:
                print("tool_call")
                if tool_calls[0]['function']['name'] == 'python':
                    print("scripting")
                    ollama_reply = tool_calls[0]['function']['arguments'].get('reply', '')
                    await message.channel.send(ollama_reply)
                    script = tool_calls[0]['function']['arguments']['script']

                    result = tool.execute_script(script)
                    print(script)
                    print(result)

                    match = re.search(r"plt\.savefig\(['\"](.*?)['\"]\)", script)

                    system_prompt = f'/no_think\nThe following is the result of your script. Regardless of the result language, use the same language as the user.{ollama_reply} {result}. Make sure to not describe any reasoning or code.'

                    async with message.channel.typing():
                        reformulation_response = await asyncio.to_thread(lambda: ollama_client.chat(
                            model=OLLAMA_MODEL,
                            messages=[
                                    {
                                        'role': 'system',
                                        'content': system_prompt
                                    }
                                ]
                        ))
                    # Extracting and storing the file path
                    if match:
                        file_path = match.group(1)
                        print("Extracted file path:", file_path)

                        try:
                            with open(file_path, "rb") as file:
                                discord_file = discord.File(file, filename="plot.png")
                                await message.channel.send(reformulation_response.get('message', {}).get('content', "The model could not generate a response."), file=discord_file)
                        except FileNotFoundError:
                            print("plot error")

                    else:
                        ollama_reply = reformulation_response.get('message', {}).get('content', "The model could not generate a response.")

                if tool_calls[0]['function']['name'] == 'memory':
                    action_type = tool_calls[0]['function']['arguments']['type']
                    user = tool_calls[0]['function']['arguments']['username']
                    query = tool_calls[0]['function']['arguments']['query']
                
                    if action_type == 'memorize':
                        print("Saving information")
                        tool.save_to_csv(f"{user},{query}")  # Store the userâ€™s data
                        print(f"{user},{query}")
                        ollama_reply = tool_calls[0]['function']['arguments'].get('reply', '')
                
                    elif action_type == 'remember':
                        print("Remembering")
                        ollama_reply = tool_calls[0]['function']['arguments'].get('reply', '')
                        await message.channel.send(ollama_reply)
                
                        # memory = tool.search_user_data(user, query)  # Retrieve stored memory
                        memory = memory_tool.load_dataset(5, user, query)
                        print(memory)
                
                        system_prompt = f"/no_think\nFetched memory: {memory}. (Only keeps the usefull one, don't even mention the other one unless relevent: {prompt})"
                
                        async with message.channel.typing():
                            reformulation_response = await asyncio.to_thread(lambda: ollama_client.chat(
                                model=OLLAMA_MODEL,
                                messages=[
                                    {
                                        'role': 'system',
                                        'content': system_prompt
                                    }
                                ]
                            ))
                        ollama_reply = reformulation_response.get('message', {}).get('content', "The model could not generate a response.")

                if tool_calls[0]['function']['name'] == 'browse':
                    action_type = tool_calls[0]['function']['arguments']['type']
                    print("browsing online")
                    ollama_reply = tool_calls[0]['function']['arguments'].get('reply', '')
                    await message.channel.send(ollama_reply)

                    conversation_history[channel_id].append({
                        'role': 'assistant',
                        'content': ollama_reply
                    })

                    query = tool_calls[0]['function']['arguments'].get('query', '')
                    if action_type == 'web':
                        print("web")
                        if query:
                            while query:
                                search_results = tool.get_search_results(query)
                                print(search_results)
                                # Reformulate the response with search results
                                reformulation_prompt = "\n".join(search_results)

                                system_prompt = f'{query}, The following is the result of your own web browsing. If you send LINK you must always use hyperLINK with the format of [title](LINK). Stay in character. Regardless of the result language, use the same language as the user. {reformulation_prompt} If there is no result or you would like to fetch the content of a website, retry using a LINK. Stay in character!'
                                async with message.channel.typing():
                                    reformulation_response = await asyncio.to_thread(lambda: ollama_client.chat(
                                        model=OLLAMA_MODEL,
                                        messages=[
                                            {
                                                'role': 'tool',
                                                'content': system_prompt
                                            }
                                        ],
                                        tools=[{
                                           'type': 'function',
                                           'function': {
                                               'name': 'web',
                                               'description': 'Get online information',
                                               'parameters': {
                                                   'type': 'object',
                                                   'properties': {
                                                       'URL': {
                                                           'type': 'string',
                                                           'description': 'Fetch the content from a obtained website',
                                                       },
                                                   },
                                                   'required': ['URL'],
                                               },
                                           },
                                        },]
                                    ))
                                    ollama_reply = reformulation_response.get('message', {}).get('content', "The model could not generate a response.")

                                    query = tool_calls[0]['function']['arguments'].get('URL', '')
                    elif action_type == 'gif':
                        print("gif")
                        gif_LINK = tool.get_gif_LINK(query)
                        print(gif_LINK)
                        ollama_reply = gif_LINK
                    elif action_type == 'youtube':
                        print("youtube")
                        search_results = tool.get_youtube_video(query)
                        # pass the reply to the llm again, but no loop
                        reformulation_prompt = "\n".join(search_results)

                        system_prompt = f"{query}, The following is the result of your own youtube browsing. If you send LINK you must always use hyperLINK with the format of [title](LINK). Stay in character. Regardless of the result language, use the same language as the user. Here's the youtube video LINK, make sure to not misspell it: {reformulation_prompt} Stay in character!"
                        print(search_results)
                        async with message.channel.typing():
                            reformulation_response = await asyncio.to_thread(lambda: ollama_client.chat(
                                model=OLLAMA_MODEL,
                                messages=[
                                    {
                                        'role': 'tool',
                                        'content': system_prompt
                                    }
                                ],
                                tools=[{
                                   'type': 'function',
                                   'function': {
                                       'name': 'web',
                                       'description': 'Get online information',
                                       'parameters': {
                                           'type': 'object',
                                           'properties': {
                                               'URL': {
                                                   'type': 'string',
                                                   'description': 'Fetch the content from a obtained website',
                                               },
                                           },
                                           'required': ['URL'],
                                       },
                                   },
                                },]
                            ))
                            ollama_reply = reformulation_response.get('message', {}).get('content', "The model could not generate a response.")
                    else:
                        ollama_reply = "No query provided for browsing."
            else:
                ollama_reply = response.get('message', {}).get('content', "Ollama did not provide a response.")
        except Exception as e:
            ollama_reply = f"An error occurred while querying Ollama: {e}"

        ollama_new_reply = ollama_reply
        ollama_reply = re.sub(r'<think>.*?</think>\n?', '', ollama_new_reply, flags=re.DOTALL)
        match = re.search(r'<think>(.*?)</think>', ollama_reply, flags=re.DOTALL)
        if match:
            print(match.group(1))
        try: # Send the reply
            if len(ollama_reply) > 2000:
                message_chunks = tool.split_message(ollama_reply, max_length=2000)
                for chunk in message_chunks:
                    await message.channel.send(chunk)
            else:
                await message.channel.send(ollama_reply)

            # Append Ollama's response to conversation history
            conversation_history[channel_id].append({
                'role': 'assistant',
                'content': ollama_reply
            })

        except Exception as e:
            await message.channel.send(f"An error occurred while sending the message: {e}")

        tool.save_conversation_history(conversation_history)

    else:
        low_importance_messages.append(prompt)
        print("ignoring")

client.run(TOKEN)